import gradio as gr
import ollama
import sys
import yaml

from pathlib import Path
from typing import Iterator


# è¯»å–é»˜è®¤é…ç½®
CONFIG_PATH: str = (Path(__file__).parent / "config.yaml").as_posix()

try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        DEFAULT_CONFIG: dict = yaml.safe_load(f)
        print("[INFO] config.yaml read successfully")
except Exception as e:
    print("[ERROR] Failed to read config.yaml")
    sys.exit(1)


# è¯»å–é»˜è®¤ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT_PATH: str = (Path(__file__).parent / "system_prompt.md").as_posix()

try:
    with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
        DEFAULT_SYSTEM_PROMPT: str = f.read()
        print("[INFO] system_prompt.md read successfully")
except Exception as e:
    print("[ERROR] Failed to read system_prompt.md")
    sys.exit(1)


# åˆ›å»º Ollama å®¢æˆ·ç«¯ï¼Œå¹¶æ£€æŸ¥ Ollama æœåŠ¡ç«¯æ˜¯å¦å¯è®¿é—®
OLLAMA_CLIENT: ollama.Client = ollama.Client(
    host=f"http://{str(DEFAULT_CONFIG["ip"])}:{int(DEFAULT_CONFIG["port"])}",
    headers={"x-some-header": "some-value"},
)

try:
    OLLAMA_CLIENT.list()
    print(f"[INFO] Ollama service is available")
except Exception as e:
    print(f"[ERROR] Ollama service is unavailable: {e}")
    sys.exit(1)


class Memory:
    """
    è®°å¿†æ¨¡å—ï¼šç®¡ç†å¯¹è¯å†å²
    """

    def __init__(self) -> None:
        self._system_prompt: dict = {"role": "system", "content": ""}
        self._messages: list[dict[str, str]] = []

    def set_system_prompt(self, prompt: str) -> None:
        """
        è®¾ç½®ç³»ç»Ÿæç¤ºè¯
        """
        self._system_prompt["content"] = prompt

    def add_user_message(self, message: str) -> None:
        """
        æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        """
        self._messages.append({"role": "user", "content": message})

    def add_ai_response(self, response: str) -> None:
        """
        æ·»åŠ  AI å›å¤
        """
        self._messages.append({"role": "assistant", "content": response})

    def get_system_prompt(self) -> dict[str, str]:
        """
        è·å–ç³»ç»Ÿæç¤ºè¯
        """
        return self._system_prompt

    def get_history(self) -> list[dict[str, str]]:
        """
        è·å–å®Œæ•´å¯¹è¯å†å²
        """
        return self._messages.copy()

    def clear(self) -> None:
        """
        æ¸…ç©ºè®°å¿†
        """
        self._messages.clear()

    # TODO åº”è¯¥æ ¹æ® num_ctx å’Œå†å²æ¶ˆæ¯çš„é•¿åº¦æ¥ç¡®å®š n
    def get_last_n_messages(self, n: int) -> list[dict[str, str]]:
        """
        è·å–æœ€è¿‘ n æ¡æ¶ˆæ¯
        """
        return self._messages[-n:] if n > 0 else []

    def __len__(self) -> int:
        """
        è¿”å›æ¶ˆæ¯æ€»æ•°
        """
        return len(self._messages)


class OllamaLLM:
    """
    Ollama LLM æ¨¡å—
    """

    def __init__(self, client: ollama.Client) -> None:
        self._client: ollama.Client = client
        self._model: str = ""
        self._num_ctx: int = 2048
        self._temperature: float = 0.7

    def set_model(self, model: str) -> None:
        """
        è®¾ç½®æ¨¡å‹å
        """
        self._model: str = model

    def set_num_ctx(self, num_ctx: int) -> None:
        """
        è®¾ç½®ä¸Šä¸‹æ–‡çª—å£å¤§å°
        """
        self._num_ctx: int = num_ctx

    def set_temperature(self, temperature: float) -> None:
        """
        è®¾ç½®æ¸©åº¦ï¼Œæ¸©åº¦èŒƒå›´ [0, 1.0]
        """
        self._temperature: float = temperature

    def chat(self, messages: list[dict[str, str]]) -> Iterator[str]:
        """
        ç”Ÿæˆ AI å›å¤
        """
        for part in self._client.chat(
            model=self._model,
            options={"num_ctx": self._num_ctx, "temperature": self._temperature},
            messages=messages,
            stream=True,
        ):
            yield part["message"]["content"]


def create_memory():
    """
    ä¸ºä¼šè¯å•ç‹¬åˆ›å»º Memory å®ä¾‹
    """

    memory: Memory = Memory()
    memory.set_system_prompt(DEFAULT_SYSTEM_PROMPT)

    return memory


def create_ollama_llm():
    """
    ä¸ºä¼šè¯å•ç‹¬åˆ›å»º OllamaLLM å®ä¾‹
    """

    ollama_llm: OllamaLLM = OllamaLLM(OLLAMA_CLIENT)
    ollama_llm.set_model(DEFAULT_CONFIG["model"]["name"])
    ollama_llm.set_num_ctx(DEFAULT_CONFIG["model"]["options"]["num_ctx"])
    ollama_llm.set_temperature(DEFAULT_CONFIG["model"]["options"]["temperature"])

    return ollama_llm


def chat_stream(
    message: str, memory: Memory, ollama_llm: OllamaLLM
) -> Iterator[tuple[list[dict[str, str]], Memory, OllamaLLM]]:
    """
    è¾“å…¥ç”¨æˆ·æ¶ˆæ¯ï¼Œç”Ÿæˆ LLM çš„å›ç­”æµï¼Œå¹¶è¿”å› Chatbot éœ€è¦çš„è¾“å…¥
    """

    memory.add_user_message(message)
    history: list[dict[str, str]] = memory.get_history()

    yield (history, memory, ollama_llm)

    history.append({"role": "assistant", "content": ""})
    messages: list[dict[str, str]] = [memory.get_system_prompt()] + history

    for word in ollama_llm.chat(messages):
        history[-1]["content"] += word

        yield (history, memory, ollama_llm)

    memory.add_ai_response(history[-1]["content"])

    yield (history, memory, ollama_llm)


def clear_chat_history(memory: Memory) -> tuple[list, Memory]:
    """
    æ¸…ç©ºå¯¹è¯å†å²
    """

    memory.clear()

    # æ¸…ç©º Chatbot æ˜¾ç¤ºå’Œå¯¹è¯è®°å¿†
    return ([], memory)


def activate_button(text: str) -> gr.Button:
    """
    å¦‚æœè¾“å…¥çš„æ–‡æœ¬ä¸ä¸ºç©ºï¼Œåˆ™æ¿€æ´»æŒ‰é’®
    """
    return gr.Button(interactive=bool(text.strip()))


def save_settings(
    system_prompt: str,
    num_ctx: int,
    temperature: float,
    memory: Memory,
    ollama_llm: OllamaLLM,
) -> tuple[Memory, OllamaLLM]:
    """
    ä¿å­˜è®¾ç½®
    """

    # å¦‚æœç³»ç»Ÿæç¤ºè¯ä¸ºç©ºï¼Œåˆ™é‡‡ç”¨é»˜è®¤çš„ç³»ç»Ÿæç¤ºè¯
    if not system_prompt.strip():
        system_prompt = DEFAULT_SYSTEM_PROMPT

    memory.set_system_prompt(system_prompt)
    ollama_llm.set_num_ctx(num_ctx)
    ollama_llm.set_temperature(temperature)

    gr.Info("Settings saved")

    return (memory, ollama_llm)


def reset_settings(
    memory: Memory, ollama_llm: OllamaLLM
) -> tuple[str, int, float, Memory, OllamaLLM]:
    """
    é‡ç½®è®¾ç½®
    """

    memory.set_system_prompt(DEFAULT_SYSTEM_PROMPT)
    ollama_llm.set_num_ctx(DEFAULT_CONFIG["model"]["options"]["num_ctx"])
    ollama_llm.set_temperature(DEFAULT_CONFIG["model"]["options"]["temperature"])

    gr.Info("Settings reset")

    return (
        "",
        DEFAULT_CONFIG["model"]["options"]["num_ctx"],
        DEFAULT_CONFIG["model"]["options"]["temperature"],
        memory,
        ollama_llm,
    )


# éšè— Chatbot å³ä¸Šè§’çš„â€œåƒåœ¾æ¡¶â€å›¾æ ‡
CSS = """
button[title="æ¸…ç©ºå¯¹è¯"] {
    display: none !important;
}
"""


with gr.Blocks(title="Ollama Chat", css=CSS) as demo:
    # è®°å¿†æ¨¡å—æ¯ä¸ªä¼šè¯ä¸€ä¸ªå®ä¾‹
    memory_state: gr.State = gr.State(value=create_memory)

    # Ollama LLM æ¨¡å—æ¯ä¸ªä¼šè¯ä¸€ä¸ªå®ä¾‹
    ollama_llm_state: gr.State = gr.State(value=create_ollama_llm)

    # ä»‹ç»
    intro: gr.HTML = gr.HTML("<center><h1>ğŸ¤– Chatbot based on Ollama</h1></center>")

    # èŠå¤©ç•Œé¢è®¾è®¡
    with gr.Tab("Chat"):
        chat_history_windows: gr.Chatbot = gr.Chatbot(type="messages", show_label=False)

        input_textbox: gr.Textbox = gr.Textbox(
            label="Input field", lines=5, max_lines=10
        )

        with gr.Row():
            send_button: gr.Button = gr.Button(value="Send", interactive=False)
            clear_button: gr.Button = gr.Button(value="Clear")

        # ä¸´æ—¶å˜é‡ï¼Œç‚¹å‡»å‘é€æŒ‰é’®åï¼Œåœ¨æ¸…ç©ºè¾“å…¥æ¡†å‰ä¿å­˜è¾“å…¥æ¡†å†…å®¹
        tmp_text: gr.Text = gr.Text(visible=False)

    # èŠå¤©åŠŸèƒ½å®ç°
    input_textbox.change(
        fn=activate_button, inputs=[input_textbox], outputs=[send_button]
    )

    send_button.click(
        fn=lambda x: ("", x),
        inputs=[input_textbox],
        outputs=[input_textbox, tmp_text],
    ).then(
        fn=chat_stream,
        inputs=[tmp_text, memory_state, ollama_llm_state],
        outputs=[chat_history_windows, memory_state, ollama_llm_state],
    ).then(
        fn=lambda: "", inputs=None, outputs=[tmp_text]
    )

    clear_button.click(
        fn=clear_chat_history,
        inputs=[memory_state],
        outputs=[chat_history_windows, memory_state],
    ).then(fn=lambda: "", inputs=None, outputs=[input_textbox])

    # è®¾ç½®ç•Œé¢è®¾è®¡
    with gr.Tab("Settings"):
        system_prompt_textbox: gr.Textbox = gr.Textbox(
            label="System prompt",
            info="System prompt is a hidden instruction preset for a LLM, guiding it to generate responses that meet expectations.",
            lines=5,
            max_lines=10,
        )

        num_ctx_slider: gr.Slider = gr.Slider(
            label="Size of context windows",
            info="Sets the size of the context window used to generate the next token.",
            value=DEFAULT_CONFIG["model"]["options"]["num_ctx"],
            minimum=1024,
            maximum=128 * 1024,
            step=1024,
        )

        temperature_slider: gr.Slider = gr.Slider(
            label="Temperature",
            info="The temperature of the model. Increasing the temperature will make the model answer more creatively.",
            value=DEFAULT_CONFIG["model"]["options"]["temperature"],
            minimum=0.0,
            maximum=1.0,
            step=0.1,
        )

        with gr.Row():
            save_button: gr.Button = gr.Button(value="Save")
            reset_button: gr.Button = gr.Button(value="Reset")

    # è®¾ç½®åŠŸèƒ½å®ç°
    save_button.click(
        fn=save_settings,
        inputs=[
            system_prompt_textbox,
            num_ctx_slider,
            temperature_slider,
            memory_state,
            ollama_llm_state,
        ],
        outputs=[memory_state, ollama_llm_state],
    )

    reset_button.click(
        fn=reset_settings,
        inputs=[memory_state, ollama_llm_state],
        outputs=[
            system_prompt_textbox,
            num_ctx_slider,
            temperature_slider,
            memory_state,
            ollama_llm_state,
        ],
    )

if __name__ == "__main__":
    FAVICON_PATH: str = (Path(__file__).parent / "ollama-logo.png").as_posix()
    demo.launch(favicon_path=FAVICON_PATH)
